# Data Science

---

### Classical Types of Data

**Categorical**

- nominal
  * values have names (describe the categories, classes, states)
  * marital status, drink type, or some binary attribute
  * cannot compare easily, thus cannot naturally order them
- ordinal
  * values have names (describe the categories, classes, states)
  * however there is ordering over the values
  * lacks a mathematical notion of distance between the values

**Numerical**

- interval
  * scale with fixed but arbitrary interval
  * the difference/distance between values is meaningful
  * cannot compute ratio or scales
- ratio
  * same as interval, but the *ratio* between items can observed unlike interval
  * there is a "zero" absolute

### Data Manipulation and Computation

**Data Representation** what is the natural way to think about given data

**Data Processing Operations** which take one or more datasets as input and produce one or more datasets as output

### Collecting Data

Five ways to get data:

- Direct download and load from local storage
- generate locally via downloaded code (e.g. simulation)
- query data from a database
- create a new entry or objectquery an API from the intra/internet
- scrape data from a webpage

**REST API's**

- **GET**: perform query, return data
- **POST**: create a new entry or object
- **PUT**: update an existing entry or object
- **DELETE**: delete an existing entry or object

Will be **stateless**, meaning that with every request you send a token/authentication of who you are.

What does a GET request return?

- General Structured Data
  - Comma-Separated Value (CSV)
  - Javascript Object NOtation (JSON)
  - HTML, XHTML, XML files & strings
- Domain-Specific Structured Data
  - Shapefiles: geospatial vector data (OpenStreetMap)
  - RVT files: architectural planning

**GRAPHQL**

- an alternative to REST and ad-hoc webservice architectures

- allows a user to request a specific structure for the data they are requesting
- makes the workload heavier server-side

**JSON Files & Strings**

- **serialize** means to convert an object into a string
- **deserialize** means to convert a string back into an object
- easy to read and powerful

**Document Object Model (DOM)**

tree-structured

---

### Groups

**Grouping** allows the regex matcher to keep track of certain portions (surrounded by parentheses) - of the match

---

### Version Control System

**Git**

- repository system for both local and remote changes
- teammates can work simultaneously on a project
- keeps track of who changed what (blame)
- allows for merging and branching operation

Goals:

- speed
- simple design
- support for non-linear development (parallel branches)
- fully distributed - not a requirement, can be centralized
- handle large projects like Linux kernel more efficiently

Is significantly smaller in size than SVN

More secure than SVN

Data Assurance - checksum is performed on upload and download to ensure the file has not been corrupted

Four states of a file:

- Modified
- Staged
- Committed
- Untracked

---

### Missing Data

**Complete Case Analysis**: delete tuples with any missing values

**Least Squares Regressions**: minimize the difference between the true Y-values and the estimated Y-values

**MCAR (Missing Completely at Random)**: when probability is not dependent on **any** of the data, then it is MCAR

**MAR (Missing at Random)**: probability of missing data is dependent on the observed data but not the unobserved data

- we can **model** the data based on why it is missing
- **Imputation**: replace the missing data with predicted values

**MNAR (Missing not at Random)**: the reason it is missing has a direct link to the data itself  - *(not testable)*



**Single Imputation**

- **Mean imputation**: imputing the average from observed cases for all missing values of a variable
- **Hot-deck imputation**: imputing a value from another observation that is most like the current subject in terms of observed variables
- **Cold-deck imputation**: bring in other datasets

These strategies impose too much precision.

**Bayes' Theorem** for predicting mising data

$P(B|A) = \frac{P(A|B) * P(B)}{P(A)}$

for our usage:

$P(H|E) = \frac{P(E|H) * P(H)}{P(E)}$

---

### Data Integration

- discovering information sources
- cleaning the data
- gathering data
- querying the data
- data mining & analyzing

Goal: Combine data from different sources and provide users with a unified view of data for querying or analysis

Two Different Setups:

1. Bring data into single repository (data warehousing)
   - easier - only need one-way mappings
   - query performance is predictable and under your control
2. Keep data where it is, and send queries back and forth
   - need two-way mappings - a query on the mediated schema needs to be translated into queries over data source schemas
   - better fit for dynamic data, but not as clean or efficient
   - could be used if warehousing is not possible

---

### Data Analysis

Extreme observations distort means but not medians

Outlying observations distort the mean

P-Standardization: observations are assigned a number from 0-100 to indicate the percentage of observations below it

---

### Graphs

Three main ways to represent a graph in memory:

1. adjacency lists
2. adjacency dictionaries
3. adjacency matrix

**Vertex Value**

Centrality Analysis:

- find out the most important node(s) in one network

Degree Centrality: importance is determined by number of adjacent vertices  